\section{Statistics} \label{sec:perfstats}
It is impossible to provide universal performance numbers for the
components of the ChannelArchiver toolset. Tests of a realistic setup
are always influenced by network delays: IOCs communicate with ArchiveEngines,
data client tools query data from the network data server.
And while the archiver tools of course share the CPU with all the
other applications that happen to run on the same CPU,
the CPU speed is less important. Most crucial is the hard
disk performance. Access to data on NFS-mounted disks is by orders of
magnitude slower than access to data on local disks.
Hard disk access is also hard to reproduce: At least under Linux, the second
run of a test is always faster because the operating system caches the
disk access. In general, the fewer files and the smaller the involved
files are, the better as far as speed is concerned, because the
operating system will cache access to files as long as memory allows.

The RTree is a balanced tree. Mathematically, this means that the
number of read requests required to locate a node in an RTree depends
on the height of the tree, which is again logarithmically related to
$M$ and the number of nodes in the tree.  An RTree with $M$=50 and
height 5 for example has one root node with 50 pointers to sub-nodes,
then up to $50^2$ nodes on the second level and so on, resulting in
access to more than $10^{8}$ records on the fifth level, that is:
with only 5 reads requests to the disk.
% 50^5 = 312500000 = 3e8
In practice, however, there can be a big time difference between 5
read requests to a file of 10~MB total size compared to 5 read
requests to a file of 500~MB total size, because the former could be
completely buffered by the operating system, while access to the
latter will result in individual disk access operations.

The following are performance values obtained on a computer
with a 1~GHz CPU, an ordinary IDE disk, that was mostly idle
while the archiver tools ran. % That's blitz.ta53.lanl.gov
The corresponding values on a machine with an 800~MHz CPU, concurrently
used by other people, but faster hard disks (Mylex
DAC960PTL1 PCI RAID Controller with 5 Quantum Atlas 10K drives) were
slightly better. % That's bogart.ta53.lanl.gov

We also provide some comparison to the previous architecture that used
the same data file format but instead of the RTree-based index there
were ``Directory Files'': A channel name hash table with pointers to
the very first and last data block.

\subsection{Write Performance}
As a baseline for raw data writing speed, the 'bench' program that can
be found in the ChannelArchiver/Engine directory consistently writes
at least 80000 values per second on the test computer.

\subsection{Index Performance}
\begin{figure}[htb]
\begin{center}
\InsertImage{width=\textwidth}{singleidxcompare}
\end{center}
\caption{\label{fig:singleidxcompare}RTree $M$ value tuning for small index, see text.}
\end{figure}

% Stats from March 2004 ORNL Xmtr test archive (RFQ, .. DTL 6)
Performance and size of the index depend on the $M$ value configuration of
the RTree. Fig.\ \ref{fig:singleidxcompare} displays the file size and
the time needed to create an index for a small archive with 5100
channels. The samples occupy 8400 data blocks in a 12~MB data file.
The ArchiveIndexTool was used to convert the existing index file of
the archive into new indices with different $M$ values.

From the number of channels and data blocks it follows that the
samples for most channels occupy only one or two data blocks.
Consequently almost all channels can be handled by degenerated RTrees, each
with a single node that is both root and leaf of the tree, using only
1 or 2 records in that node. Any records beyond the first few 
remain unused. Fig.\ \ref{fig:singleidxcompare} clearly indicates how
the file size grows linearly with $M$ due to those unused records.
The changes in the time needed to create an index can probably be
explained as follows: After creating the first new index with
$M=3$, the time dropped observably because the operating system would from now
on cache most read requests to the original index. With growing $M$,
the time again increases caused by the growing file sizes of the new indices.
 
\begin{figure}[htb]
\begin{center}
\InsertImage{width=\textwidth}{masteridxcompare}
\end{center}
\caption{\label{fig:masteridxcompare}RTree $M$ value tuning for master
  index, see text.}
\end{figure}

Fig.\ \ref{fig:masteridxcompare} compares the file sizes and creation
time over $M$ of a master index that covers 27 sub-archives, a total
of 635~MB of data files containing 248261 data blocks for 6164
channels.  The smaller archive from the preceding section is actually
one sub-archive of this master index. Because some channels have only
very few samples, while other channels might have changed every 30
seconds, there cannot be one $M$ value that is ideal for every
channel handled by the master index. By creating the master index with
different values of $M$, we are looking for a compromise that gives
best index performance across channels.

Fig.\ \ref{fig:masteridxcompare} shows
that values between 10 and 50 result in a smaller master index than
$M$ values outside of this range. Remember that for a given height,
the number of leaf records in an RTree grows exponentially with $M$,
so slight increases of $M$ beyond 50 will vastly increase the number
of leaf records. Archives with twice or ten times the number of data
blocks will therefore not require $M$ values that are equally 2 or 10 times
bigger. Only very small increases of $M$ would be beneficial. If we
consider that in general those bigger archives will also contain
channels with only a few samples, $M=50$ will probably be ``as good''.
% Overall, about 1:30..4 min for 27 sub-indices totaling 50 .. 160 MB
% Master index is 18..24 MB

In the following, $M$ was kept at 50, the default for most archive tools.
\begin{itemize}
% LANL Xmtr Data 2002:
\item 12 sub-archives, 1.2~MB of old directory files, 1.4~GB Data
      files:\\
      Converting directory files into index files with $M=50$:
      Just under 3~minutes, resulting in 11~MB for the new index files.\\
      Creating a master index: 37~seconds for a master index of 9~MB.
      The master index is slightly smaller than the sum of the
      individual sub-indices because of better RTree utilization:
      The $M$ was configured to be 50 in all cases and many channels
      in the sub-archives use only a fraction of a single RTree
      node, down to an average record usage of 8\%, while the master
      index uses around 50\%.
      A re-run of the ArchiveIndexTool tool is faster because
      it detects data block that are already listed in the master index and
      therefore not added again. In this case, the re-run took 10~seconds.
% LANL Xmtr Data 2003:
\item 92 sub-archives, 12~MB directory files, 2.3GB of data files:\\
      Converting into 61~MB of index files: About 12 minutes.\\
      Creating a master index: Under 2~minutes, the resulting
      index uses about 18~MB.
      Re-run: 30~seconds.
\end{itemize}

\subsection{Impact of Data Management on Performance}
As a less-than-perfect example, we created a collection of mostly
hourly sub-archives, resulting in 297 sub-archives, 158~MB index
files, 307~MB data files.  Creation of a master index took about
25~minutes, resulting in a master index file size of 65~MB.  A re-run
of the Index Tool took about 3~minutes.

By combining the hourly sub-archives into monthly ones, the count was
reduced from 297 sub-archives to only 16.  This took about
8~minutes, resulting in 5.5~MB for index files and 148~MB for data
files. Creation of a master index for the 16 sub-archives now took
26~seconds, a re-run was further reduced to 1.5~seconds.
Overall this shows that periodic data management, combining individual
sub-archives into fewer ones, will reduce not only the number of files
but also file sizes, resulting in better performance.

\subsection{Binary Index compared to list index}
The following data was actually taken with the old ``multi archive'',
which is basically equivalent to the current ``list'' index which
simply uses a linear list of sub-archives without any further
optimization.

%  ORNL March 2004 Xmtr archive:
% dir. files were about 1MB each, 27 of them.
% 0.9 sec  : ArchiveExport index3 -i -m Tnk.:T$
% 1.8 sec  : ArchiveExport index100 -i -m Tnk.:T$
% 7..8 secs: ArchiveManager {one of the 27 dir. files} -m Tnk.:T$
% 4 min    : ArchiveManager master_file -m Tnk.:T$
% ArchiveExport all data for 6 tank temperature channels:
% 11.336 secs, 85819 lines -> 7570 lines/sec
% (that's including spreadsheet 'filling'
% Similar, but only for one channel:
% 0.566 secs, 21166 lines -> 37000 lines/sec

We took a test archive consisting of 638 small sub-archives,
where the data files totaled only a little over 400~MB.
A master index was created as well as a ``multi archive'' file
that lists the 638 sub-archives.
Creation of that master index too 15~min.OB
\begin{itemize}
\item Time to list all  540 channel names: $<$1~second.\\
      This takes 40 seconds with the old ``multi archive'' file.
      The results for finding names that match a pattern or
      determining the available time range for a channel are
      similar.

      The new index is clearly superior in this test case simply
      because the data is contained in one index, while the ``multi
      archive'' file required access to all 638 sub-archives.
\item The time to retrieve a few samples from the start, middle or end of the
      archive is fairly constant around 0.1 seconds with the new
      index. With the previous implementation, the lookup time for
      the samples depends on where the respective sub-archive is
      positioned in the multi-archive file. In one test it ranged from
      0.1 to 10 seconds.  When the data is found in the first few
      sub-archives listed in the multi-archive file, the times compare
      to the new index. The further one goes down the list of
      sub-archives as they appeared in the multi-archive file, the
      longer it takes. A reproducible test is difficult because the
      preceding tests (list all channels) causes the operating system
      to cache many of the sub-archive's directory files.
\end{itemize}

\subsection{Retrieval Performance}
Tests of the retrieval performance often include not only the
code for getting at the data but also for presenting it. In the
case of the command-line Archive Export program this would be the process
of converting time stamps and values into ASCII text and printing them.
In the following tests, the output was redirected to a file.
\begin{itemize}
\item Use ArchiveExport to dump all the 143000 values for a channel:
      4~seconds, translating into 35700 values per second.
\item Use ArchiveExport to dump all the values of one month for 7
      channels in a spreadsheet format, which adds the effort for
      'staircase' interpolation to the previous test case:
      Each channel had 20000 to 30000 values. The interpolation generated
      a 95000 line spreadsheet in 11 secs, that is around 8600 lines/sec.
      On a second test run, the time was reduced to 7 seconds, again
      showing the impact of buffering by the operating system.
\item Use Matlab to retrieve the first 500 raw samples of the same 7 channels:
      The ArchiveServer ran 0.7 seconds, Matlab used a total of 1.7
      seconds from sending the request to receiving the data.
%     7*500/0.7 : 5000 samples/sec., probably governed by initial
%     lookup. network etc. add 1 sec.
\item Use Matlab to retrieve data for the same 7 channels,
      asking the network data server to reduce the raw data (which
      formed the 95000 line spreadsheet in the previous test) into
      500 ``Plot Bins'':
      Around 1500 values per channel were retrieved in 5 seconds.
      The ArchiveDataServer.cgi ran 3 seconds, so about two
      seconds were added by the web server, network transfer, Matlab
      MEX code and Matlab.
%     So 7*1500/5:  2100 values/sec. 2 sec overhead for network.
\item Use the Java Archive Client to retrieve the same 7 channels into 800
      ``Plot Bins'':
      Around 2100 values per channel were retrieved and plotted in 12 seconds.
      (When using the alternate machine with the slower CPU but RAID disks as 
      the data server, the time was reduced to 8 seconds).
      The Java client was still usable, but slow to respond with this
      amount of data: Zoom requests took about 2 seconds.
\end{itemize}

This shows that the initial lookup of a channel and the location
of the samples in the data files requires a certain time.
Reading the values can then be quite fast and reach more than 30000
values per second when simply fetching the raw samples.
Interpolation or binning can internally reach this speed when
investigating the raw data, 

The network data server typically adds about 1 second of overhead.
