\section{Binary Index Files, RTree}
A binary index file contains a list of all the channels in an archive, and
for each channel it contains information about the data blocks which
are available in the Data Files of an archive.
The archiver toolset uses index files for two slightly different
purposes:
\begin{enumerate}
\item Each ArchiveEngine creates an index for the data files
      that it writes.
      We refer to this combination of index and data files
      as a \INDEX{Sub-Archive}.
      If a sub archive contains data for a certain channel and time
      range, it will contain that data only once.
\item We can create a \INDEX{Master Index} that points to data
      in several sub archives.
      Several sub-archives might contain data for the same channel
      and time range. When we combine sub-archives into a master
      index, we can assign \INDEX{Sub-Archive Priorities} to
      determine what data is considered more important.
\end{enumerate}

\noindent Another important difference between sub-archive index files
and master index files lies in the fact that the sub-archive index
files only the names to their data files: The sub-archive index
resides in the same directory as its data files, so a path name is not
required to get from the index to the data files.
A sub-archive index and its data file can be moved to a new
location. As long as the index file and its data files remain
together in one directory, the location of that directory does not matter.

The master index file on the other hand contains the path names to its
data files, because different sub-archives can use the same data file
names within their sub-archive directory. We can only distinguish these
data files by their full path.  Once a master archive index has been
created, the sub-archives must therefore not be moved. After
relocating any of the sub-archives, the master index needs to be recreated.

Inside the index file, the channel names are maintained in a hash
table and the data block information is kept in a modified RTree
structure.  An RTree \cite{guttman84} is a balanced tree tailored for
holding multidimensional data like rectangles, allowing lookup of
rectangles via points that fall inside rectangles.  Sergei Chevtsov
extended this concept to handle time ranges by requiring that the leaf
node entries are non-overlapping and sorted in time.

Each RTree node consists of several records. How many records there are
per node is determined by a tunable parameter $M$:
The archiver tools use $M$ as the upper limit of records
per node, i.e.\ a node will simply contain up to $M$ records.
(The literature often applies $M$ in a slightly different way,
where nodes contain up to $2M-1$ records.)
The records in the leave nodes of the tree point to data block
information (i.e.\ path to a data file and offset inside that data
file) and the time range that is covered by the data block.
The records do not overlap, i.e.\ no two records will cover the same
time range, and the records are sorted in time.
Since the actual data blocks might overlap (at least for a master index), more
than one non-overlapping record might refer to the same data block.
Records in parent nodes reflect the time range covered by all their
child node records, up to the root node records which hold the total
time range covered by all the data blocks.

\begin{figure}[htb]
\begin{center}
\InsertImage{width=\textwidth}{indices_a_and_b}
\end{center}
\caption{\label{fig:indices}RTree Demo, refer to text.}
\end{figure}

\begin{figure}[htb]
\begin{center}
\InsertImage{width=\textwidth}{index_ab}
\end{center}
\caption{\label{fig:masterindex}RTree Demo, refer to text.}
\end{figure}

Fig.~\ref{fig:indices} demonstrates two trees with $M=3$. The one to the left
covers the total time range from ``1'' to ``7'' (in the real world,
these numbers would be much bigger since they represent seconds since
some ``Epoch'').  The data for the time range from e.g.\ ``3'' to
``4'' can be found in data file ``FileA'' at offset 0x30. To handle a
request for a time range [3;6], we first determine if that range is
covered by the tree at all by checking the root's time range. Since
that is the case, we go down one level, check the sub-nodes, go down
again etc.\ until we end up at the data blocks.

Fig.~\ref{fig:masterindex} demonstrates how the two trees from
Fig.~\ref{fig:indices} would be combined into a Master Index, assuming
that the tree for FileA resides in directory dir\_a and the data for
the second sub-archive resides in dir\_b relative to the master index.
Note how all the data blocks now include a path together with a file
name. Since the sub-archive for FileA was listed first in the
configuration of the master index, its data blocks take precedence
over those from FileB whenever there is an overlap.

The examples used a small number for $M$ so that one can see the tree
structure even though we only have a few data blocks. Bigger values of
$M$ will reduce the number of read and write operations because the
tree is accessed node by node, reading respectively writing all $M$
records of a node in one system call. A big $M$ value will also reduce the
height of the tree, as well as the number of nodes and read/write
calls. On the other hand, the size of a node obviously grows with $M$,
and the time for reading respectively writing a single node can
slightly increase. In general, the number of read/write system calls
has a bigger impact on the performance than the size of the individual
reads/writes.
The records within a node are accessed via a linear search over all
the records in a node. A bigger $M$ will require slightly more CPU
time for this linear search, which again is neglegible compared to the
time required for disk access.
Overall, a bigger $M$ is likely to increase performance because it
reduces the number of disk accesses.
The mayor drawback to a big $M$ results from possible fragmentation:
If you create many small sub-archives, each tree of the sub-archive
will only contain few entries. The minimum size of the tree as well as
the size increment whenever a new node needs to be added is determined
by $M$. Big values of $M$ can result in a lot of unused space in the
index file, creating unnecessarily big index files.
Section \ref{sec:perfstats} will present some quantitative details on
the performance of the RTree index.

\subsection{Implementation Details}

\begin{table}[htbp]
  \begin{center}
    \sffamily
    \begin{tabular}{ll}
     Offset  & Content \\
     \hline
     0x0000  & 'CAI2' \\
     0x0004  & NameHash anchor: start (0x30), size N \\
     0x000C  & FileAllocator used list: size, start (0x24), end \\
     0x0018  & FileAllocator free list: size, start, end \\
     0x0024  & FileAllocator header: size, prev(0), next(??) \\
     0x0030  & Hash Table Entry 0: \\
             & Pointer to first entry for this hash value \\
     0x0034  & Hash Table Entry 1:  \\
     ...     & ... \\
     ??????  & Hash Table Entry N-1:  \\
     ...     & ... \\
     ??????  & FileAllocator header: size, prev(0), next(??) \\
     ??????  & Hash Entry: \\
             & next, RTree pointer, channel name \\
     ...     & ... \\  
     ??????  & FileAllocator header: size, prev(0), next(??) \\
     ??????  & RTree: pointer to root node, M value \\
     ...     & ... \\  
    \end{tabular}
    \caption{Index file: Example layout.}
    \label{tab:indexfile}
  \end{center}
\end{table}

\noindent Table~\ref{tab:indexfile} shows the basic layout of an index
file.  The header of the index file contains a 4-ASCII character magic
id like 'CAI2' for ``Channel Archiver Index Type 2'', and the hash
table anchor.  Those 12 bytes constitute ``reserved space'' for the
FileAllocator class. What follows is start- and end pointers for the
FileAllocator's list of allocated and available items, because the
remaining file space is handled by the FileAllocator class.  The first
allocated region is the NameHash, so it's start location would be
known. Each hash table entry points to the start of channel entries
that hashed to the respective value, and each channel entry contains
the anchor for its RTree.  The ``RTree pointer'' in the Hash Entry is
actually a file name and an offset. Initially, that file name is
empty, because all RTrees are in the same index file that contains the
hash table. Eventually, that index file might get too big, so the file
format already allows for RTree entries to point to other index files.
Like every file block after the ``reserved space'', the hash table and
each channel entry are preceded by a FileAllocator header.

An RTree entry consists of the pointer to the root node and a number
of records per node $M$. The RTree nodes are interlinked as shown in
the example in Fig.~\ref{fig:indices} and \ref{fig:masterindex}, where
each node and data block is allocated from the FileAllocator class.
For details of how the nodes and data blocks are written to the disk,
please refer to the source code.
